{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe0f484-391b-4d75-9863-46135953c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers safetensors pandas numpy scikit-learn scipy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086677f8-d8c3-4b2b-8cbd-171ea57d4d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae743718-062c-4653-9a64-1b16fa771ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall torch torchvision torchaudio -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a046dae-a87b-4720-9376-451b944ade90",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4b9d85-0c89-4018-8266-3ff893b74b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import torch; print(torch.__version__); print(torch.cuda.is_available())\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95160e3-6d85-40f4-a17c-820f7257e7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from safetensors.torch import load_file\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix)\n",
    "from scipy.stats import bootstrap\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2567926f-7f4d-43ca-8fc7-53c9f53ae896",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f5b36d-1091-4c19-8c9e-c634ac748361",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"bert_base_v2_Jun26\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained(model_dir, state_dict=None)\n",
    "state_dict = load_file(f\"{model_dir}/model.safetensors\")\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f39336-2de3-4247-b979-9baccfccc518",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"DATA-batch-2-comparison-2-final-for-NLP-classifier-JC-allData-v2.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec839a73-77ac-49ff-87f9-41f087799014",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df[\"Text_snippet\"].to_list()\n",
    "labels = df[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b232a119-f5e4-45d3-9b91-bc9b46ee1bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c3cdf-58d4-421a-923c-ce320bc459d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(texts):\n",
    "    return tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9168748-fea7-46d7-9f4b-dd352465c58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, texts, batch_size=16):\n",
    "    preds, probs = [], []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = encode(batch)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probs_batch = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "            preds_batch = (probs_batch > 0.5).astype(int)\n",
    "        preds.extend(preds_batch)\n",
    "        probs.extend(probs_batch)\n",
    "    return np.array(preds), np.array(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f70702-c274-4453-b9ae-f736321267c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train, y_proba_train = predict(model, X_train)\n",
    "y_pred_test, y_proba_test = predict(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0239abb9-e573-43da-a48c-bb603cd8cc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train set with predictions\n",
    "df_train = pd.DataFrame({\n",
    "    \"Text_snippet\": X_train,\n",
    "    \"labels\": y_train,\n",
    "    \"preds\": y_pred_train,\n",
    "    \"probs\": y_proba_train\n",
    "})\n",
    "df_train.to_csv(\"train_with_preds.csv\", index=False)\n",
    " \n",
    "# Save test set with predictions\n",
    "df_test = pd.DataFrame({\n",
    "    \"Text_snippet\": X_test,\n",
    "    \"labels\": y_test,\n",
    "    \"preds\": y_pred_test,\n",
    "    \"probs\": y_proba_test\n",
    "})\n",
    "df_test.to_csv(\"test_with_preds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bcddd0-9e51-4e09-a652-23459b2a1708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use original dataframe indices during train/test split\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    texts, labels, df.index, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439e6902-2b50-4e28-87a3-ea71f992fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train, y_proba_train = predict(model, X_train)\n",
    "y_pred_test, y_proba_test = predict(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc12d2f-1aa9-4d76-bd48-c53ad246c9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"DATA-batch-2-comparison-2-final-for-NLP-classifier-JC-allData-v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2834a417-a80a-4cde-a0e7-418b37f4bd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make empty columns\n",
    "\n",
    "df[\"preds\"] = np.nan\n",
    "df[\"probs\"] = np.nan\n",
    "df[\"split\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcc0666-822e-4776-8ca0-1748c340507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to it\n",
    "\n",
    "df.loc[idx_train, \"preds\"] = y_pred_train\n",
    "df.loc[idx_train, \"probs\"] = y_proba_train\n",
    "df.loc[idx_train, \"split\"] = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae17ec4c-a27e-455d-9c11-40777b22e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx_test, \"preds\"] = y_pred_test\n",
    "df.loc[idx_test, \"probs\"] = y_proba_test\n",
    "df.loc[idx_test, \"split\"] = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddaa14e-1cb9-45a1-9dc9-e4672d89bc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"all_data_v2_with_preds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03ef593-13c8-4266-85b3-35f6cfa5ef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93201e5d-2e11-424c-a74a-4fe9edb0dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713dbb07-4df0-4dea-8ef8-b40052b06ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure correct types\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "df[\"preds\"] = df[\"preds\"].astype(int)\n",
    "df[\"probs\"] = df[\"probs\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e6e1fb-7f9e-428d-a460-b8a5f3cdedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0a5921-2f13-497d-bba0-e177ce814eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_metric(y_true, y_pred, y_proba, metric_func, n_bootstrap=1000, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Calculate bootstrap confidence intervals for a given metric\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    n_samples = len(y_true)\n",
    "    bootstrap_scores = []\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        # Bootstrap sample\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        y_true_boot = y_true.iloc[indices] if hasattr(y_true, 'iloc') else y_true[indices]\n",
    "        y_pred_boot = y_pred.iloc[indices] if hasattr(y_pred, 'iloc') else y_pred[indices]\n",
    "\n",
    "        if y_proba is not None:\n",
    "            y_proba_boot = y_proba.iloc[indices] if hasattr(y_proba, 'iloc') else y_proba[indices]\n",
    "        else:\n",
    "            y_proba_boot = None\n",
    "\n",
    "        try:\n",
    "            if metric_func.__name__ == 'roc_auc_score':\n",
    "                if y_proba_boot is not None:\n",
    "                    score = metric_func(y_true_boot, y_proba_boot)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                score = metric_func(y_true_boot, y_pred_boot)\n",
    "            bootstrap_scores.append(score)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "   \n",
    "    bootstrap_scores = np.array(bootstrap_scores)\n",
    "    alpha = 1 - confidence_level\n",
    "    lower_percentile = (alpha / 2) * 100\n",
    "    upper_percentile = (1 - alpha / 2) * 100\n",
    "\n",
    "    ci_lower = np.percentile(bootstrap_scores, lower_percentile)\n",
    "    ci_upper = np.percentile(bootstrap_scores, upper_percentile)\n",
    "\n",
    "    return ci_lower, ci_upper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d5bb32-4eeb-4992-a543-5c49c5d3fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_binary_metrics(y_true, y_pred, y_proba=None, pos_label=1):\n",
    "    \"\"\"\n",
    "    Calculate binary classification metrics\n",
    "    \"\"\"\n",
    "    # Confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "   \n",
    "    # Basic metrics\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # PPV is the same as precision, sensitivity is the same as recall\n",
    "\n",
    "    ppv = precision\n",
    "    sensitivity = recall \n",
    "\n",
    "    # AUC\n",
    "    auc = roc_auc_score(y_true, y_proba) if y_proba is not None else np.nan\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'npv': npv,\n",
    "        'ppv': ppv,\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'auc': auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f740dd-3e7a-4076-a108-00dcf0bf0d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_with_ci(df, split_type='train'):\n",
    "    \"\"\"\n",
    "    Calculate metrics with confidence intervals for a specific split\n",
    "    \"\"\"\n",
    "    # Filter data by split\n",
    "    data = df[df['split'] == split_type].copy()\n",
    "\n",
    "    if len(data) == 0:\n",
    "        return None\n",
    "\n",
    "   \n",
    "    y_true = data['label']\n",
    "    y_pred = data['preds']\n",
    "    y_proba = data['probs'] if 'probs' in data.columns else None\n",
    "\n",
    "   \n",
    "    # Get unique labels\n",
    "    unique_labels = sorted(y_true.unique())\n",
    "    results = {}\n",
    "   \n",
    "    # Calculate metrics for each label\n",
    "    for label in unique_labels:\n",
    "        # Convert to binary classification (current label vs rest)\n",
    "        y_true_binary = (y_true == label).astype(int)\n",
    "        y_pred_binary = (y_pred == label).astype(int)\n",
    "\n",
    "        # Calculate base metrics\n",
    "        metrics = calculate_binary_metrics(y_true_binary, y_pred_binary, y_proba)\n",
    "\n",
    "       \n",
    "        # Calculate confidence intervals\n",
    "\n",
    "        metric_names = ['precision', 'recall', 'f1', 'npv', 'ppv', 'sensitivity', 'specificity']\n",
    "        metric_funcs = {\n",
    "            'precision': lambda yt, yp: precision_score(yt, yp, zero_division=0),\n",
    "            'recall': lambda yt, yp: recall_score(yt, yp, zero_division=0),\n",
    "            'f1': lambda yt, yp: f1_score(yt, yp, zero_division=0),\n",
    "            'npv': lambda yt, yp: calculate_binary_metrics(yt, yp)['npv'],\n",
    "            'ppv': lambda yt, yp: precision_score(yt, yp, zero_division=0),  # Same as precision\n",
    "            'sensitivity': lambda yt, yp: recall_score(yt, yp, zero_division=0),  # Same as recall\n",
    "            'specificity': lambda yt, yp: calculate_binary_metrics(yt, yp)['specificity']\n",
    "        }\n",
    "       \n",
    "        label_results = {}\n",
    "\n",
    "        for metric_name in metric_names:\n",
    "            ci_lower, ci_upper = bootstrap_metric(\n",
    "                y_true_binary, y_pred_binary, y_proba, metric_funcs[metric_name]\n",
    "            )\n",
    "\n",
    "            label_results[metric_name] = {\n",
    "                'value': metrics[metric_name],\n",
    "                'ci_lower': ci_lower,\n",
    "                'ci_upper': ci_upper,\n",
    "                'ci_formatted': f\"{metrics[metric_name]:.3f} ({ci_lower:.3f}-{ci_upper:.3f})\"\n",
    "            }\n",
    "\n",
    "       \n",
    "        # AUC (only if probabilities are available)\n",
    "\n",
    "        if y_proba is not None:\n",
    "            try:\n",
    "                ci_lower, ci_upper = bootstrap_metric(\n",
    "                    y_true_binary, y_pred_binary, y_proba, roc_auc_score\n",
    "                )\n",
    "\n",
    "                label_results['auc'] = {\n",
    "                    'value': metrics['auc'],\n",
    "                    'ci_lower': ci_lower,\n",
    "                    'ci_upper': ci_upper,\n",
    "                    'ci_formatted': f\"{metrics['auc']:.3f} ({ci_lower:.3f}-{ci_upper:.3f})\"\n",
    "                }\n",
    "\n",
    "            except:\n",
    "                label_results['auc'] = {\n",
    "                    'value': np.nan,\n",
    "                    'ci_lower': np.nan,\n",
    "                    'ci_upper': np.nan,\n",
    "                    'ci_formatted': \"N/A\"\n",
    "                }      \n",
    "\n",
    "        results[f'label_{label}'] = label_results\n",
    "\n",
    "   \n",
    "\n",
    "    # Calculate weighted averages\n",
    "\n",
    "    weighted_results = {}\n",
    "    metric_names = ['precision', 'recall', 'f1', 'npv', 'ppv', 'sensitivity', 'specificity', 'auc']\n",
    "\n",
    "\n",
    "    for metric_name in metric_names:\n",
    "        if metric_name == 'auc' and y_proba is None:\n",
    "            continue\n",
    "\n",
    "        # Calculate weighted average\n",
    "\n",
    "        label_counts = y_true.value_counts()\n",
    "        total_count = len(y_true)     \n",
    "        weighted_sum = 0\n",
    "        for label in unique_labels:\n",
    "            weight = label_counts[label] / total_count\n",
    "            metric_value = results[f'label_{label}'][metric_name]['value']\n",
    "            if not np.isnan(metric_value):\n",
    "                weighted_sum += weight * metric_value\n",
    "\n",
    "        # Bootstrap for weighted average CI\n",
    "\n",
    "        def weighted_metric_func(y_true_sample, y_pred_sample):\n",
    "            label_counts_sample = pd.Series(y_true_sample).value_counts()\n",
    "            total_count_sample = len(y_true_sample)\n",
    "            weighted_sum_sample = 0\n",
    "           \n",
    "            for label in unique_labels:\n",
    "                if label in label_counts_sample:\n",
    "                    weight = label_counts_sample[label] / total_count_sample\n",
    "                    y_true_binary_sample = (y_true_sample == label).astype(int)\n",
    "                    y_pred_binary_sample = (y_pred_sample == label).astype(int)\n",
    "\n",
    "\n",
    "                    if metric_name == 'precision':\n",
    "                        metric_value = precision_score(y_true_binary_sample, y_pred_binary_sample, zero_division=0)\n",
    "                    elif metric_name == 'recall':\n",
    "                        metric_value = recall_score(y_true_binary_sample, y_pred_binary_sample, zero_division=0)\n",
    "                    elif metric_name == 'f1':\n",
    "                        metric_value = f1_score(y_true_binary_sample, y_pred_binary_sample, zero_division=0)\n",
    "                    elif metric_name in ['npv', 'ppv', 'sensitivity', 'specificity']:\n",
    "                        metric_value = calculate_binary_metrics(y_true_binary_sample, y_pred_binary_sample)[metric_name]\n",
    "\n",
    "                    weighted_sum_sample += weight * metric_value\n",
    "\n",
    "            return weighted_sum_sample\n",
    "\n",
    "        if metric_name != 'auc':\n",
    "            ci_lower, ci_upper = bootstrap_metric(y_true, y_pred, None, weighted_metric_func)\n",
    "        else:\n",
    "            # For AUC, we need a different approach since it's calculated differently\n",
    "            try:\n",
    "                ci_lower, ci_upper = bootstrap_metric(y_true, y_pred, y_proba,\n",
    "                                                   lambda yt, yp: roc_auc_score(yt, yp))\n",
    "            except:\n",
    "                ci_lower, ci_upper = np.nan, np.nan\n",
    "\n",
    "        weighted_results[metric_name] = {\n",
    "            'value': weighted_sum,\n",
    "            'ci_lower': ci_lower,\n",
    "            'ci_upper': ci_upper,\n",
    "            'ci_formatted': f\"{weighted_sum:.3f} ({ci_lower:.3f}-{ci_upper:.3f})\"\n",
    "        }\n",
    "\n",
    "    results['weighted_average'] = weighted_results\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e60966c-283d-48f3-bd05-a558f84fae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to calculate all metrics\n",
    "\n",
    "def calculate_all_metrics(df):\n",
    "    \"\"\"\n",
    "    Calculate all metrics for both train and test splits\n",
    "    \"\"\"\n",
    "    # Validate required columns\n",
    "    required_cols = ['label', 'preds', 'split']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Calculate for train split\n",
    "\n",
    "    train_results = calculate_metrics_with_ci(df, 'train')\n",
    "    if train_results:\n",
    "        results['train'] = train_results\n",
    "\n",
    "    # Calculate for test split\n",
    "\n",
    "    test_results = calculate_metrics_with_ci(df, 'test')\n",
    "    if test_results:\n",
    "        results['test'] = test_results\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa3ce5b-32bf-49d2-b1ee-4cdbd1b77146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display results in a formatted way\n",
    "\n",
    "def display_results(results):\n",
    "    \"\"\"\n",
    "    Display results in a formatted table\n",
    "    \"\"\"\n",
    "    for split_name, split_results in results.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"RESULTS FOR {split_name.upper()} SET\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Display results for each label\n",
    "\n",
    "        for label_key, label_results in split_results.items():\n",
    "            if label_key == 'weighted_average':\n",
    "                continue\n",
    "\n",
    "            print(f\"\\n{label_key.upper()}:\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "            for metric_name, metric_data in label_results.items():\n",
    "                print(f\"{metric_name.upper():>12}: {metric_data['ci_formatted']}\")\n",
    "\n",
    "        # Display weighted averages\n",
    "\n",
    "        if 'weighted_average' in split_results:\n",
    "            print(f\"\\nWEIGHTED AVERAGES:\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "            for metric_name, metric_data in split_results['weighted_average'].items():\n",
    "                print(f\"{metric_name.upper():>12}: {metric_data['ci_formatted']}\")\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38280cd-806e-47c9-9008-94388b2db7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all metrics\n",
    "\n",
    "results = calculate_all_metrics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c8e253-79fc-44aa-bb19-a4f1de15b3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "\n",
    "display_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b131156-ddb5-42f2-9de1-e3031f3c6dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore auc above - inconsistent and probailities are likely inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b640b8ef-3e2b-425b-998d-9def25040873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519c7466-a386-4ad3-a808-a1f5fdbb438c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
