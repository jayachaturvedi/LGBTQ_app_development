{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJPJNXTJuVsS",
    "tags": []
   },
   "source": [
    "# Initial imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U scikit-learn==1.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.22.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfZffjko4dHn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "\n",
    "# import spacy\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "#import nltk\n",
    "from collections import Counter\n",
    "\n",
    "#from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "#from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSGW-nh5-GfE",
    "tags": []
   },
   "source": [
    "# Data for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1qBDOWMRUGm"
   },
   "source": [
    "Load the data for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OAWygjHuhqzT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2XLH1_R4eWj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_for_classifier = pd.read_csv(path+'allData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "FwGUExRK4kis",
    "outputId": "916e7088-697a-4070-cbe8-87222d958e02",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check first 5 rows\n",
    "\n",
    "data_for_classifier.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check label distribution\n",
    "\n",
    "data_for_classifier['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate lengths of spans\n",
    "data_for_classifier['Text_snippet_len'] = data_for_classifier['Text_snippet'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find minimum, maximum, and mean lengths - update column name to text column of the df\n",
    "min_length = data_for_classifier['Text_snippet_len'].min()\n",
    "max_length = data_for_classifier['Text_snippet_len'].max()\n",
    "mean_length = data_for_classifier['Text_snippet_len'].mean()\n",
    "median_length = data_for_classifier['Text_snippet_len'].median()\n",
    "\n",
    "# Print results\n",
    "print(f\"Minimum Length: {min_length}\")\n",
    "print(f\"Maximum Length: {max_length}\")\n",
    "print(f\"Mean Length: {mean_length}\")\n",
    "print(f\"Median Length: {median_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTJf8cu5RXqu",
    "tags": []
   },
   "source": [
    "# Split the data into train and test (80:20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5QaX57tGOT4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data, testing_data = train_test_split(data_for_classifier, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0vB36IF7GULX",
    "outputId": "fc351d51-567f-4eda-c9c9-5f6dc479a032",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"No. of training examples: {training_data.shape[0]}\")\n",
    "print(f\"No. of testing examples: {testing_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "bGjrekCqGWyL",
    "outputId": "fe2278ea-376e-4221-f4aa-0b026315d936",
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y113XixbGitL",
    "outputId": "b788390b-4ab4-4dd0-9874-9bfff45b178f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check label counts\n",
    "\n",
    "training_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check label percentages\n",
    "\n",
    "training_data['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "LqrL8YRlGacf",
    "outputId": "d8b2e07b-ee9a-42a1-d8e9-a38eecd91385",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check first 5 rows\n",
    "\n",
    "testing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save test data as a separate file for use later\n",
    "\n",
    "testing_data.to_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AxrZBSBgGgBV",
    "outputId": "712f64d7-0f56-4b56-a0a3-f7fd82c08107",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check label counts\n",
    "\n",
    "testing_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check label percentages\n",
    "\n",
    "testing_data['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klq3iD_y-K43",
    "tags": []
   },
   "source": [
    "# BERT Classification 1\n",
    "\n",
    "Run the second BERT classification below for confidence intervals etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSW39FULGy2P"
   },
   "source": [
    "Install and import BERT specific packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install - q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3SpcqUfwjBs",
    "outputId": "1aac346d-f17c-40d8-9428-81edbdb125e3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check for GPU using tensorflow. If present it will say something like Found GPU at: /device:GPU:0\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install - q torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MnivD2i-wlkd",
    "outputId": "4d5dc288-47e1-4015-e5b5-3b548e51d939",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check for GPU using torch. If present it will say something like There are 1 GPU(s) available. We will use the GPU: Tesla T4\n",
    "\n",
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YXy9E_HiGybM",
    "outputId": "8f3bee93-fca2-4e84-c8e3-d241285d7577",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install - q transformers==4.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DpmjFihGplE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm import trange\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizerFast, BertTokenizer, AdamW, AutoModel, AutoModelForSequenceClassification, AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR bert_base_uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHhbX1FI_uWg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "BERT_tokenizer = 'bert-base-uncased'\n",
    "\n",
    "MAX_TKN_LEN=511"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284,
     "referenced_widgets": [
      "ad5b6b6f43864a219bb5b573375e4459",
      "d53b0be5499a4560ad318ddefdfb94ec",
      "86fe03383ae347708284fbd4d4ad4659",
      "d8993681a7274d7699ad112cf8a5142f",
      "d70f2b0dd02d444f8b883896e1351b8f",
      "4403a81740bd47bebc8a3c330090a297",
      "5670d4b4e8894ff38b338518c37e3ec1",
      "438a048d82a74b84acd8b27a2f9073cd",
      "7999f514a67b4b8a91c9e7dfb1c5db9a",
      "18032b86ea1f465385792529ba1578af",
      "60a21d70be2c4fa9abe8b376cfb5f41f",
      "4bcece6c94f54573bb2c0a925fd3cddc",
      "8595bbbf16c94f37a297e35cac21264b",
      "c9004b62a6644142ad7c398be1cdee9b",
      "cf5382d3c2ed4feabb63e7c9a33d09b9",
      "32cd4c65e8b046b1bf664ca0dc983136",
      "32903a36fc1a450fbc409ac0b6b5f3b1",
      "0f237212ac864b3294f5cb3fddeb9fb7",
      "1670eef7f5264c8193441ad4d00e77f1",
      "6b4816403a39414588ffe530732862c9",
      "0cd2c99160f8438480f4b0a65afea8ba",
      "d51957f0c02f457c9c80c3e112bf58c2",
      "407c40cd8c284d10b5b3b5699888b611",
      "ddd73e659c134b07aca49a04a3973f52",
      "425ea3669a7446a8ad5f76fd0beca284",
      "c6415b02b9254825b9e924fed751774a",
      "abaed86415d348da9bc38c0b10f2ed49",
      "987c7ebb9b1a45149ddc9e050d965e88",
      "67f2428d5ace47fbbdb6b88b1b87ef22",
      "db3dd9d756ab4e419557a1a5fedd3de5",
      "06b221ddb5fd49249040f1e55b74d228",
      "b0c1aea6a8934007a582a01dcef83cd4",
      "0c8cc7297c2240a2b882436d9d20b9f9",
      "492ae1237b724366aadfaf083c132784",
      "5abe141de9944255ba3b000546152c2d",
      "65bb3aa2c4da4f05b67334880a8c82ac",
      "fa4547987b4649dfb0654d938672e9d3",
      "f34e3b80a1fb42138af5bbdd1d437272",
      "d6c6377d50d646b1b489a2074a39183c",
      "6c13f58d2f0a47ca9d23383c45d0fc54",
      "b48d90386454488f98c7d733654fca2e",
      "7feb965a03d54caab36b47dc80192d1b",
      "6d923a2acb064161850fdb740e8cfaa7",
      "6776154723f54c9d92f3ade98426607c",
      "59f9889cf43a43e19a67067495887400",
      "bd3eda2c1abb4f49b454a6dbb6af93ec",
      "093b07e2911746a7b4a5760bbf983003",
      "f951c4a9618548cbb57f388966ebf034",
      "8a9ab6e29aa44ed0a1666b54ab8f5c09",
      "dc4efddb180d4f17945f7ee700d10044",
      "85ee92792a9247ba813ff6d3e35e50f3",
      "768827fd1bec42f599c9a127e622b016",
      "57c72f3ef0e24c45853fbe5ac944fac2",
      "bce10f3bb5584faab6ab8d1d33ccccd1",
      "874c1416b7034814929f54fa099e0bc8"
     ]
    },
    "id": "vJi5tQ4lHKx5",
    "outputId": "054805ac-1240-4e41-80f7-e4202864823a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import BERT-base pretrained model\n",
    "\n",
    "trained_bert_model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "trained_bert_model.cuda()\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "8fKQKL7JAVPq",
    "outputId": "99035001-1219-4f4b-d7c0-90dae7346f5d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_hxuuUeCfpj",
    "outputId": "b0c7c6da-23d0-4441-8894-09ca3c821aa1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdzUrGukDc8l",
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_col = 'Text_snippet'\n",
    "label_col = 'label'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QEYZqha9HQo2",
    "outputId": "2638636e-bdd1-409c-953f-926e3f61ef30",
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data['Text_snippet'] = training_data['Text_snippet'].apply(lambda x: x.strip().lower())\n",
    "training_data['Text_snippet']= training_data['Text_snippet'].str.replace('[^\\w\\s]','')\n",
    "print('num annotations:', len(training_data), '\\n\\n', training_data['label'].value_counts(), '\\n\\n', training_data[['label', 'Text_snippet']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BERT functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCVZBCk7EdBo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the DataLoaders for our training and validation sets.\n",
    "# For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n",
    "def create_dataloader(train_dataset, val_dataset, batch_size=16):\n",
    "    # We'll take training samples in random order.\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,  # The training samples.\n",
    "        sampler=RandomSampler(train_dataset),  # Select batches randomly\n",
    "        batch_size=batch_size  # Trains with this batch size.\n",
    "    )\n",
    "    # For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "    validation_dataloader = DataLoader(\n",
    "        val_dataset,  # The validation samples.\n",
    "        sampler=SequentialSampler(val_dataset),  # Pull out batches sequentially.\n",
    "        batch_size=batch_size  # Evaluate with this batch size.\n",
    "    )\n",
    "\n",
    "    return [train_dataloader, validation_dataloader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4aXZLThlEc-O",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert labels to numerical values\n",
    "def convert_to_cat(labels, binary=False):\n",
    "    if not isinstance(labels, pd.Series):\n",
    "        labels = pd.Series(labels)\n",
    "    if binary:\n",
    "        main_val = labels.mode()[0]\n",
    "        other_val = [x for x in labels.unique() if x != main_val]\n",
    "        labels = np.where(labels != main_val, 1, 0)\n",
    "        corresp = pd.DataFrame([main_val, 'other'], columns=['old_label'])\n",
    "    elif np.issubdtype(labels.dtype, np.number) and (labels.min() >= 0):\n",
    "        print('labels already in a good format')\n",
    "        return {'labels': labels, 'categories': pd.DataFrame(labels.unique())}\n",
    "    else:\n",
    "        cats = labels.astype('category').cat\n",
    "        labels = cats.codes.astype('long')  # convert annotations to integers\n",
    "        corresp = pd.DataFrame(cats.categories, columns=['old_label'])\n",
    "    corresp.index.rename('new_label', inplace=True)\n",
    "    print('labels have been transformed for the model:\\n\\n', corresp)\n",
    "    return {'labels': labels, 'categories': corresp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N6TFG7b_Ec7P",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to calculate performance of our predictions vs labels\n",
    "def perf_metrics(preds, labels, average='weighted', debug=False):\n",
    "    try:\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "        # pred_flat = torch.max(pred_vec, 1)[1]\n",
    "    except:\n",
    "        print('only 1 dimension in labels prediction, no need for argmax')\n",
    "        pred_flat = preds.flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    acc = accuracy_score(labels_flat, pred_flat)\n",
    "    f1 = f1_score(labels_flat, pred_flat, average=average)\n",
    "    p = precision_score(labels_flat, pred_flat, average=average)\n",
    "    r = recall_score(labels_flat, pred_flat, average=average)\n",
    "    if debug:\n",
    "        print(\"PERF -- Acc: {:.3f} F1: {:.3f} Precision: {:.3f} Recall: {:.3f}\".format(acc, f1, p, r))\n",
    "    return {'f1': f1, 'acc': acc, 'p': p, 'r': r}\n",
    "\n",
    "\n",
    "def perf_metrics_classes(preds, labels):\n",
    "    try:\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    except:\n",
    "        print('only 1 dimension in labels prediction, no need for argmax')\n",
    "        pred_flat = preds.flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    report = classification_report(labels_flat, pred_flat, output_dict=True)\n",
    "    df = pd.DataFrame(report).sort_index().transpose()\n",
    "    # df['accuracy'] = accuracy_score(labels, preds)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMi678-FEc4v",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# functions to handle different devices\n",
    "def to_cpu(vec, detach=True):\n",
    "    try:\n",
    "        vec = vec.detach().cpu().numpy() if detach else vec.to('cpu').numpy()\n",
    "    except:\n",
    "        vec = vec.detach().numpy() if detach else vec.numpy()\n",
    "    return vec\n",
    "\n",
    "\n",
    "def model_to_cpu(model):\n",
    "    try:\n",
    "        model.to(\"cpu\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def batch_to_gpu(batch, device=None):\n",
    "    if device is None:\n",
    "        return batch\n",
    "    try:  # we're using a GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "    except:\n",
    "        batch = tuple(t for t in batch)\n",
    "    return batch\n",
    "\n",
    "def get_device():\n",
    "    # specify GPU device\n",
    "    try:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if device != 'cpu':\n",
    "            print('number GPUs used:', torch.cuda.device_count())\n",
    "            print('device name:', torch.cuda.get_device_name(0))\n",
    "    except:\n",
    "        device = None\n",
    "        print('no CUDA capable device detected')\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_yidGJpBEc1g",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prep_BERT_dataset(sentences, labels=None, BERT_tokenizer='bert-base-uncased', MAX_TKN_LEN=511, debug=False):\n",
    "    \"\"\"\n",
    "    JC: change tokenizer to bert-base-uncased or 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext'\n",
    "    \n",
    "    prepares data for BERT fine-tuning\n",
    "    :param sentences: list or array-like. list of texts to classify\n",
    "    :param labels: list or array-like, default None. list of labels corresponding to sentences\n",
    "    :param BERT_tokenizer: string, default 'bert-base-uncased'. BERT base model used\n",
    "    :param MAX_TKN_LEN: integer, default 511. see https://github.com/huggingface/transformers/issues/2446\n",
    "    :param debug: bool, default False. set to True to display inetrmediate results\n",
    "    :return: prepared dataset (torch Dataset) and nmber of dictinct labels\n",
    "    \"\"\"\n",
    "    sentences = pd.Series(sentences)\n",
    "    # load relevant data and add special tokens for BERT to work properly\n",
    "    sentences = [\"[CLS] \" + query + \" [SEP]\" for query in sentences]\n",
    "    if labels is not None:\n",
    "        labels = convert_to_cat(labels, binary=False)['labels']\n",
    "    else:\n",
    "        labels = pd.Series([1] * len(sentences))\n",
    "    if debug: print(sentences[0])\n",
    "\n",
    "    # Tokenize with BERT tokenizer\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(BERT_tokenizer, do_lower_case=True)\n",
    "    if MAX_TKN_LEN is not None:\n",
    "        print('cutting the length of tokens to', MAX_TKN_LEN)\n",
    "        tokenized_texts = [tokenizer.tokenize(sent)[0:MAX_TKN_LEN] for sent in sentences]\n",
    "    else:\n",
    "        tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts]\n",
    "    if debug:\n",
    "        print(\"Tokenize the first sentence:\")\n",
    "        print(len(tokenized_texts), len(input_ids), len(input_ids[0]))\n",
    "        print(tokenized_texts[0], input_ids[0])\n",
    "\n",
    "    # add paddding to input_ids\n",
    "    input_ids_padded = pad_sequence([torch.tensor(i) for i in input_ids]).transpose(0, 1)\n",
    "    if debug: print(input_ids_padded.size(), len(input_ids_padded))\n",
    "\n",
    "    # Create attention masks\n",
    "    attention_masks = []\n",
    "    # Create a mask of 1s for each token followed by 0s for padding\n",
    "    for seq in input_ids_padded:\n",
    "        seq_mask = [float(i > 0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "\n",
    "    # create dataset\n",
    "    dataset = TensorDataset(input_ids_padded, torch.tensor(attention_masks), torch.tensor(labels))\n",
    "    num_labels = labels.nunique()\n",
    "    return {'dataset': dataset, 'num_labels': num_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjJ5i1DfEcyo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def finetune_BERT_pytorch(model, train_dataloader, validation_dataloader, n_epochs=3, output_dir='./bert_base_v2_Jun26/'):\n",
    "    \"\"\"\n",
    "    JC: update output_dir or say None\n",
    "    \n",
    "    fine-tunes Bert for text classification\n",
    "    :param model: base Bert model\n",
    "    :param train_dataloader: Tensor Dataset. training data\n",
    "    :param validation_dataloader: Tensor Dataset. testing data\n",
    "    :param n_epochs: integer, default 5. number of epochs\n",
    "    :param output_dir: string, default None. directory where to save the fine-tuned model\n",
    "    :return:    model: fine-tuned Bert model with highest performance over k folds\n",
    "                stats: high level performance statistics\n",
    "                stats_classes: detailed performance statistics\n",
    "    \"\"\"\n",
    "    ###################################################################################\n",
    "    # BERT fine-tuning parameters\n",
    "    device = get_device()\n",
    "    try:\n",
    "        model.cuda()\n",
    "    except:\n",
    "        device = None\n",
    "        print('using CPU, this will be slow!')\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\n",
    "\n",
    "    # BERT training loop\n",
    "    train_loss_set = []\n",
    "    best_f1, best_epoch = 0, 0\n",
    "    for _ in trange(n_epochs, desc=\"Epoch\"):\n",
    "        ###################################################################################\n",
    "        ## TRAINING\n",
    "\n",
    "        # Set our model to training mode\n",
    "        model.train()\n",
    "        # Tracking variables\n",
    "        tr_loss, tr_perf, tr_perf_classes = 0, Counter({}), pd.DataFrame()\n",
    "        running_len = 0\n",
    "        # Train the data for one epoch\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Unpack the inputs from our dataloader (and move to GPU if using)\n",
    "            batch = batch_to_gpu(batch, device)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            # Clear out the gradients (by default they accumulate)\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss, logits = outputs['loss'], outputs['logits']\n",
    "            train_loss_set.append(loss.item())\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Update parameters and take a step using the computed gradient\n",
    "            optimizer.step()\n",
    "            # Update tracking variables\n",
    "            tr_loss += loss.item()\n",
    "            tmp_tr_perf = perf_metrics(to_cpu(logits), to_cpu(b_labels), average='weighted')\n",
    "            tmp_tr_perf.update((k, v * len(b_input_ids)) for k, v in tmp_tr_perf.items())\n",
    "            running_len += len(b_input_ids)\n",
    "            tr_perf = tr_perf + Counter(tmp_tr_perf)\n",
    "            tmp_tr_perf_classes = perf_metrics_classes(to_cpu(logits), to_cpu(b_labels))\n",
    "            tr_perf_classes = pd.concat((tr_perf_classes, tmp_tr_perf_classes))\n",
    "\n",
    "        # print('classes detail \\n\\n', tr_perf_classes)\n",
    "        tr_perf = {k: v / running_len for k, v in tr_perf.items()}\n",
    "        tr_perf_classes[['f1-score', 'precision', 'recall']] = tr_perf_classes[\n",
    "            ['f1-score', 'precision', 'recall']].multiply(tr_perf_classes['support'], axis=\"index\")\n",
    "        tr_perf_classes = tr_perf_classes.groupby(tr_perf_classes.index).sum()\n",
    "        tr_perf_classes[['f1-score', 'precision', 'recall']] = tr_perf_classes[['f1-score', 'precision', 'recall']].div(\n",
    "            tr_perf_classes['support'], axis=\"index\")\n",
    "        print('TRAIN - Loss: {:.3f} - F1: {:.3f} Acc: {:.3f} P: {:.3f} R: {:.3f}'.format(tr_loss / (1 + step),\n",
    "                                                                                         tr_perf['f1'], tr_perf['acc'],\n",
    "                                                                                         tr_perf['p'], tr_perf['r']))\n",
    "\n",
    "        ###################################################################################\n",
    "        ## VALIDATION\n",
    "\n",
    "        # Put model in evaluation mode\n",
    "        model.eval()\n",
    "        # Tracking variables\n",
    "        eval_perf, eval_perf_classes = Counter({}), pd.DataFrame()\n",
    "        running_len = 0\n",
    "        # Evaluate data for one epoch\n",
    "        for step, batch in enumerate(validation_dataloader):\n",
    "            # Unpack the inputs from our dataloader (and move to GPU if using)\n",
    "            batch = batch_to_gpu(batch, device)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            with torch.no_grad():  # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "                # Forward pass, calculate logit predictions\n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "                loss, logits = outputs['loss'], outputs['logits']\n",
    "            # Update tracking variables\n",
    "            tmp_eval_perf = perf_metrics(to_cpu(logits), to_cpu(b_labels), average='weighted')\n",
    "            tmp_eval_perf.update((k, v * len(b_input_ids)) for k, v in tmp_eval_perf.items())\n",
    "            # print('STEP:', step, 'LEN', len(b_input_ids), tmp_eval_perf)\n",
    "            running_len += len(b_input_ids)\n",
    "            eval_perf = eval_perf + Counter(tmp_eval_perf)\n",
    "            tmp_eval_perf_classes = perf_metrics_classes(to_cpu(logits), to_cpu(b_labels))\n",
    "            eval_perf_classes = pd.concat((eval_perf_classes, tmp_eval_perf_classes))\n",
    "\n",
    "        eval_perf = {k: v / running_len for k, v in\n",
    "                     eval_perf.items()}  # eval_perf = {k:v/(1+step) for k,v in eval_perf.items()}\n",
    "        eval_perf_classes[['f1-score', 'precision', 'recall']] = eval_perf_classes[\n",
    "            ['f1-score', 'precision', 'recall']].multiply(eval_perf_classes['support'], axis=\"index\")\n",
    "        eval_perf_classes = eval_perf_classes.groupby(eval_perf_classes.index).sum()\n",
    "        eval_perf_classes[['f1-score', 'precision', 'recall']] = eval_perf_classes[\n",
    "            ['f1-score', 'precision', 'recall']].div(eval_perf_classes['support'], axis=\"index\")\n",
    "        print('TEST -- F1: {:.3f} Acc: {:.3f} P: {:.3f} R: {:.3f}'.format(eval_perf['f1'], eval_perf['acc'],\n",
    "                                                                          eval_perf['p'], eval_perf['r']))\n",
    "\n",
    "        # store perf metrics and model\n",
    "        if eval_perf['f1'] >= best_f1:\n",
    "            best_f1 = eval_perf['f1']\n",
    "            best_epoch = _ + 1\n",
    "            stats_to_save = eval_perf\n",
    "            tr_perf_classes['dataset'] = 'train'\n",
    "            eval_perf_classes['dataset'] = 'test'\n",
    "            stats_classes_to_save = pd.concat([tr_perf_classes, eval_perf_classes])\n",
    "\n",
    "        if output_dir is not None: #save model with best F1\n",
    "            try:\n",
    "                print('saving...')\n",
    "                model.save_pretrained(output_dir)\n",
    "                stats_classes_to_save.to_csv(output_dir + '/stats.csv', header=True)\n",
    "            except:\n",
    "                print('model not saved, please enter valid path')\n",
    "        print('best F1 score obtained: {:.3f} at epoch {}'.format(best_f1, best_epoch))\n",
    "\n",
    "    return {'stats': stats_to_save, 'stats_classes': stats_classes_to_save, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JH2hf8gOEcwB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_BERT(model, train_dataloader, validation_dataloader, n_epochs=3, output_dir='./bert_base_v2_Jun26/'):\n",
    "    \"\"\"\n",
    "    JC: update output_dir or say None\n",
    "    JC: This is where we can implement weighted crossentropy loss for class imbalance\n",
    "    \n",
    "    fine-tunes Bert for text classification\n",
    "    :param model: base Bert model\n",
    "    :param train_dataloader: Tensor Dataset. training data\n",
    "    :param validation_dataloader: Tensor Dataset. testing data\n",
    "    :param n_epochs: integer, default 5. number of epochs\n",
    "    :param output_dir: string, default None. directory where to save the fine-tuned model\n",
    "    :return:    model: fine-tuned Bert model with highest performance over k folds\n",
    "                stats: high level performance statistics\n",
    "                stats_classes: detailed performance statistics\n",
    "    \"\"\"\n",
    "    ###################################################################################\n",
    "    # BERT fine-tuning parameters\n",
    "    device = get_device()\n",
    "    try:\n",
    "        model.cuda()\n",
    "    except:\n",
    "        #device = None\n",
    "        print('could not move Bert to Cuda, using CPU, this will be slow!')\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\n",
    "    #loss_fn = nn.CrossEntropyLoss() #JC try this?\n",
    "    \n",
    "    # weighted cross entropy loss for class imbalance\n",
    "    class_weights = torch.tensor([0.82, 0.17], device=device)  # Modify these weights accordingly [wt of class0, wt of class1] - To TRY with lgbtq\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights) # To TRY with lgbtq\n",
    "    \n",
    "    # BERT training loop\n",
    "    train_loss_set = []\n",
    "    best_f1, best_epoch = 0, 0\n",
    "    for _ in trange(n_epochs, desc=\"Epoch\"):\n",
    "        ###################################################################################\n",
    "        ## TRAINING\n",
    "\n",
    "        # Set our model to training mode\n",
    "        model.train()\n",
    "        # Tracking variables\n",
    "        tr_loss, tr_perf, tr_perf_classes = 0, Counter({}), pd.DataFrame()\n",
    "        running_len = 0\n",
    "        # Train the data for one epoch\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Unpack the inputs from our dataloader (and move to GPU if using)\n",
    "            batch = batch_to_gpu(batch, device)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            # Clear out the gradients (by default they accumulate)\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            #loss, logits = outputs['loss'], outputs['logits']\n",
    "            #train_loss_set.append(loss.item()) \n",
    "            \n",
    "            #if using cross entropy then comment out loss, logits and use the lines below\n",
    "            ##loss = output[0] #keep this commented\n",
    "            \n",
    "            logits = outputs[1]\n",
    "            loss = loss_fn(logits,b_labels)\n",
    "            print(loss)\n",
    "            train_loss_set.append(loss.item())\n",
    "            \n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Update parameters and take a step using the computed gradient\n",
    "            optimizer.step()\n",
    "            # Update tracking variables\n",
    "            tr_loss += loss.item()\n",
    "            tmp_tr_perf = perf_metrics(to_cpu(logits), to_cpu(b_labels), average='weighted')\n",
    "            tmp_tr_perf.update((k, v * len(b_input_ids)) for k, v in tmp_tr_perf.items())\n",
    "            running_len += len(b_input_ids)\n",
    "            tr_perf = tr_perf + Counter(tmp_tr_perf)\n",
    "            tmp_tr_perf_classes = perf_metrics_classes(to_cpu(logits), to_cpu(b_labels))\n",
    "            tr_perf_classes = pd.concat((tr_perf_classes, tmp_tr_perf_classes))\n",
    "\n",
    "        # print('classes detail \\n\\n', tr_perf_classes)\n",
    "        tr_perf = {k: v / running_len for k, v in tr_perf.items()}\n",
    "        tr_perf_classes[['f1-score', 'precision', 'recall']] = tr_perf_classes[\n",
    "            ['f1-score', 'precision', 'recall']].multiply(tr_perf_classes['support'], axis=\"index\")\n",
    "        tr_perf_classes = tr_perf_classes.groupby(tr_perf_classes.index).sum()\n",
    "        tr_perf_classes[['f1-score', 'precision', 'recall']] = tr_perf_classes[['f1-score', 'precision', 'recall']].div(\n",
    "            tr_perf_classes['support'], axis=\"index\")\n",
    "        print('TRAIN - Loss: {:.3f} - F1: {:.3f} Acc: {:.3f} P: {:.3f} R: {:.3f}'.format(tr_loss / (1 + step),\n",
    "                                                                                         tr_perf['f1'], tr_perf['acc'],\n",
    "                                                                                         tr_perf['p'], tr_perf['r']))\n",
    "\n",
    "        ###################################################################################\n",
    "        ## VALIDATION\n",
    "\n",
    "        # Put model in evaluation mode\n",
    "        model.eval()\n",
    "        # Tracking variables\n",
    "        eval_perf, eval_perf_classes = Counter({}), pd.DataFrame()\n",
    "        running_len = 0\n",
    "        # Evaluate data for one epoch\n",
    "        for step, batch in enumerate(validation_dataloader):\n",
    "            # Unpack the inputs from our dataloader (and move to GPU if using)\n",
    "            batch = batch_to_gpu(batch, device)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            with torch.no_grad():  # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "                # Forward pass, calculate logit predictions\n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "                loss, logits = outputs['loss'], outputs['logits']\n",
    "            # Update tracking variables\n",
    "            tmp_eval_perf = perf_metrics(to_cpu(logits), to_cpu(b_labels), average='weighted')\n",
    "            tmp_eval_perf.update((k, v * len(b_input_ids)) for k, v in tmp_eval_perf.items())\n",
    "            # print('STEP:', step, 'LEN', len(b_input_ids), tmp_eval_perf)\n",
    "            running_len += len(b_input_ids)\n",
    "            eval_perf = eval_perf + Counter(tmp_eval_perf)\n",
    "            tmp_eval_perf_classes = perf_metrics_classes(to_cpu(logits), to_cpu(b_labels))\n",
    "            eval_perf_classes = pd.concat((eval_perf_classes, tmp_eval_perf_classes))\n",
    "\n",
    "        eval_perf = {k: v / running_len for k, v in\n",
    "                     eval_perf.items()}  # eval_perf = {k:v/(1+step) for k,v in eval_perf.items()}\n",
    "        eval_perf_classes[['f1-score', 'precision', 'recall']] = eval_perf_classes[\n",
    "            ['f1-score', 'precision', 'recall']].multiply(eval_perf_classes['support'], axis=\"index\")\n",
    "        eval_perf_classes = eval_perf_classes.groupby(eval_perf_classes.index).sum()\n",
    "        eval_perf_classes[['f1-score', 'precision', 'recall']] = eval_perf_classes[\n",
    "            ['f1-score', 'precision', 'recall']].div(eval_perf_classes['support'], axis=\"index\")\n",
    "        print('TEST -- F1: {:.3f} Acc: {:.3f} P: {:.3f} R: {:.3f}'.format(eval_perf['f1'], eval_perf['acc'],\n",
    "                                                                          eval_perf['p'], eval_perf['r']))\n",
    "\n",
    "        # store perf metrics and model\n",
    "        if eval_perf['f1'] >= best_f1:\n",
    "            best_f1 = eval_perf['f1']\n",
    "            best_epoch = _ + 1\n",
    "            stats_to_save = eval_perf\n",
    "            tr_perf_classes['dataset'] = 'train'\n",
    "            eval_perf_classes['dataset'] = 'test'\n",
    "            stats_classes_to_save = pd.concat([tr_perf_classes, eval_perf_classes])\n",
    "\n",
    "        if output_dir is not None: #save model with best F1\n",
    "            try:\n",
    "                print('saving...')\n",
    "                model.save_pretrained(output_dir)\n",
    "                stats_classes_to_save.to_csv(output_dir + '/stats.csv', header=True)\n",
    "            except:\n",
    "                print('model not saved, please enter valid path')\n",
    "        print('best F1 score obtained: {:.3f} at epoch {}'.format(best_f1, best_epoch))\n",
    "\n",
    "    return {'stats': stats_to_save, 'stats_classes': stats_classes_to_save, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGXDme2zEctA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_BERT(sentences, labels, BERT_tokenizer='bert-base-uncased', test_size=0.1,\n",
    "                        n_epochs=20, batch_size=16, output_dir='./bert_base_v2_Jun26/', MAX_TKN_LEN=511):\n",
    "    \"\"\"\n",
    "    JC: update output_dir or say None; \n",
    "    JC: update tokenizer to bert-base-uncased or 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext'\n",
    "    JC: Upped epochs from 3 to 20 to facilitate improved generalisability on CRIS data\n",
    "    \n",
    "    formats dataset and fine-tunes Bert model on it\n",
    "    :param sentences: list or array-like. list of texts to classify\n",
    "    :param labels: list or array-like. list of labels corresponding to sentences\n",
    "    :param BERT_tokenizer: string, default 'bert-base-uncased'. BERT base model used\n",
    "    :param test_size: float, default 0.10. train/test split size\n",
    "    :param n_epochs: integer, default 5. number of epochs used to fine-tune Bert\n",
    "    :param batch_size: integer, default 32. how many samples pper batch to load\n",
    "    :param output_dir: string, default None. directory where to save the fine-tuned model\n",
    "    :param MAX_TKN_LEN: integer, default 511. see https://github.com/huggingface/transformers/issues/2446\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print('formating dataset')\n",
    "    prep_data = prep_BERT_dataset(sentences=sentences, labels=labels, BERT_tokenizer=BERT_tokenizer,\n",
    "                                  MAX_TKN_LEN=MAX_TKN_LEN)\n",
    "    dataset = prep_data['dataset']\n",
    "    num_labels = prep_data['num_labels']\n",
    "    # split into train/test\n",
    "    print('splitting in train/test sets')\n",
    "    test_len = int(len(dataset) * test_size)\n",
    "    train_len = len(dataset) - test_len\n",
    "    print('test set:', test_len, 'train set:', train_len)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_len, test_len])\n",
    "    # Create the DataLoaders for our training and validation sets.\n",
    "    train_dataloader, validation_dataloader = create_dataloader(train_dataset, val_dataset, batch_size=batch_size)\n",
    "    # Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\n",
    "    print('loading pre-trained BERT')\n",
    "    pretrained_model = BertForSequenceClassification.from_pretrained(BERT_tokenizer, num_labels=num_labels, output_attentions = False, output_hidden_states = False)\n",
    "    # train and evaluate BERT\n",
    "    print('training BERT')\n",
    "    res = finetune_BERT_pytorch(pretrained_model, train_dataloader, validation_dataloader, n_epochs=n_epochs, output_dir=output_dir)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfN1PeAoEcnP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load pre-trained model and classify a new sentence\n",
    "def load_and_run_BERT(sentences, trained_bert_model, BERT_tokenizer='bert-base-uncased', MAX_TKN_LEN=511,\n",
    "                      batch_size=16):\n",
    "    \"\"\"\n",
    "    JC: update tokenizer to bert-base-uncased or 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext'\n",
    "    \n",
    "    loads pre-trained Bert model and runs it on new text to classify\n",
    "    :param sentences: list or array-like. list of texts to classify\n",
    "    :param trained_bert_model: string or pytorch model. pre-trained model name or path\n",
    "    :param BERT_tokenizer: string, default 'bert-base-uncased'. BERT base model used\n",
    "    :param MAX_TKN_LEN: integer, default 511. see https://github.com/huggingface/transformers/issues/2446\n",
    "    :param batch_size: integer, default 32. how many samples pper batch to load\n",
    "    :return: dataframe of sentences classified along with probaility scores\n",
    "    \"\"\"\n",
    "    if isinstance(trained_bert_model, str):  # load trained BERT model if needed\n",
    "        trained_bert_model = BertForSequenceClassification.from_pretrained(trained_bert_model)\n",
    "    preds_class, probs, preds = [], [], pd.DataFrame()\n",
    "    sentences = pd.Series(sentences)\n",
    "    sentences_dataset = \\\n",
    "    prep_BERT_dataset(sentences, labels=None, BERT_tokenizer=BERT_tokenizer, MAX_TKN_LEN=MAX_TKN_LEN)['dataset']\n",
    "    # b_input_ids, b_input_mask, b_labels = sentences_dataset.tensors\n",
    "    validation_dataloader = DataLoader(sentences_dataset, sampler=SequentialSampler(sentences_dataset),\n",
    "                                       batch_size=batch_size)\n",
    "    for step, batch in enumerate(validation_dataloader):\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = trained_bert_model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss, logits = outputs['loss'], outputs['logits']\n",
    "        preds_curr = logits.detach().numpy()\n",
    "        preds = preds.append(pd.DataFrame(preds_curr), ignore_index=True)\n",
    "        probs = np.append(probs, np.max(np.exp(preds_curr) / (1 + np.exp(preds_curr)), axis=1))\n",
    "        preds_class = np.append(preds_class, np.argmax(preds_curr, axis=1).flatten())\n",
    "\n",
    "    # put results in nice format\n",
    "    preds['sentences'] = sentences\n",
    "    preds['preds'] = preds_class\n",
    "    preds['probs'] = probs\n",
    "    preds['label_goldstd'] = df['label'] #testing something - remove this if not needed or causing errors - use only when you want to do error analysis\n",
    "    preds['brcid'] = df['brcid'] #testing something - remove this if not needed or causing errors - use only when you want to do error analysis\n",
    "    preds['CN_Doc_ID'] = df['CN_Doc_ID'] #testing something - remove this if not needed or causing errors - use only when you want to do error analysis\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdNEn7nbFbbo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TO RUN K-FOLD VALIDATION\n",
    "def run_KFOLD(dataset, base_model, model_trainer, base_model_loader=None,\n",
    "             n_splits=10, random_state=42, n_epochs=3, **kwargs):\n",
    "      \n",
    "    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    labels = pd.Series([1] * len(dataset))\n",
    "    # tracking variables\n",
    "    best_f1, fold_nb = 0, 0\n",
    "    stats, stats_classes = pd.DataFrame(), pd.DataFrame()\n",
    "    # run k fold\n",
    "    for train_ix, test_ix in kfold.split(labels, labels):\n",
    "        fold_nb +=1\n",
    "        # need to load each time otherwise remembers training from previous fold\n",
    "        if base_model_loader is not None:\n",
    "            base_model_tmp = base_model_loader(base_model, **kwargs)\n",
    "        else:\n",
    "            base_model_tmp = base_model\n",
    "        print('####################### RUNNING FOLD:', fold_nb)\n",
    "        train_dataset = torch.utils.data.Subset(dataset, train_ix)\n",
    "        val_dataset = torch.utils.data.Subset(dataset, test_ix)\n",
    "        print(type(train_dataset), ' train set:', len(train_dataset), ' test set:',len(val_dataset))\n",
    "        train_dataloader, validation_dataloader = create_dataloader(train_dataset, val_dataset)\n",
    "        print('training and evaluating model')\n",
    "        res = model_trainer(base_model_tmp, train_dataloader, validation_dataloader, n_epochs=n_epochs, output_dir=None)\n",
    "        del base_model_tmp\n",
    "        # store perf metrics and model\n",
    "        stats_tmp = pd.DataFrame.from_dict(res['stats'],orient='index', columns=['value'])\n",
    "        stats_tmp['fold'] = fold_nb\n",
    "        stats = pd.concat([stats, stats_tmp])\n",
    "        res['stats_classes']['fold'] = fold_nb\n",
    "        stats_classes = pd.concat([stats_classes, res['stats_classes']])\n",
    "        if res['stats']['f1'] >= best_f1:\n",
    "            best_f1 = res['stats']['f1']\n",
    "            res_to_save = res\n",
    "            \n",
    "        \n",
    "    print('best F1 score obtained across splits: {:.3f}'.format(best_f1))\n",
    "    return {'stats':stats, 'stats_classes':stats_classes, 'model':res_to_save['model']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_PPYfy7FbYa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def BERT_KFOLD(sentences, labels, BERT_tokenizer='bert-base-uncased',\n",
    "               n_splits=10, random_state=42, n_epochs=3, output_dir='./bert_base_v2_Jun26/', MAX_TKN_LEN=511):\n",
    "    \"\"\"\n",
    "    JC: update output_dir or say None; \n",
    "    JC: update tokenizer to bert-base-uncased or 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext'\n",
    "    \n",
    "    fine-tunes Bert model using k-old cross validation\n",
    "    :param sentences: list or array-like. list of texts to classify\n",
    "    :param labels: list or array-like. list of labels corresponding to sentences\n",
    "    :param BERT_tokenizer: string, default 'bert-base-uncased'. BERT base model used\n",
    "    :param n_splits: integer, default None. number of folds to use\n",
    "    :param random_state: integer, default 42. random seed to initialize folds generation\n",
    "    :param n_epochs: integer, default 5. number of epochs used to fine-tune Bert\n",
    "    :param output_dir: string, default None. directory where to save the fine-tuned model\n",
    "    :param MAX_TKN_LEN: integer, default 511. see https://github.com/huggingface/transformers/issues/2446\n",
    "    :return:    model: fine-tuned Bert model with highest performance over k folds\n",
    "                stats: high level performance statistics\n",
    "                stats_classes: detailed performance statistics\n",
    "    \"\"\"\n",
    "    prep_data = prep_BERT_dataset(sentences=sentences, labels=labels, BERT_tokenizer=BERT_tokenizer,\n",
    "                                  MAX_TKN_LEN=MAX_TKN_LEN)\n",
    "    dataset = prep_data['dataset']\n",
    "    res = run_KFOLD(dataset=dataset, base_model=BERT_tokenizer, model_trainer=run_BERT,\n",
    "                    base_model_loader=BertForSequenceClassification.from_pretrained,\n",
    "                    n_splits=n_splits, random_state=random_state, n_epochs=n_epochs, num_labels=prep_data['num_labels'])\n",
    "    if output_dir is not None:\n",
    "        try:\n",
    "            print('saving model in:', output_dir)\n",
    "            res['model'].save_pretrained(output_dir)\n",
    "            res['stats_classes'].to_csv(output_dir + '/stats.csv', header=True)\n",
    "        except:\n",
    "            print('model not saved, please enter valid path')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Diana's code - can do the same for precision ci and recall ci using precision_score and recall_score (first import them) instead of f1_score\n",
    "\n",
    "def f1_conf_int(target, predict, bootstrap_n = 500):\n",
    "    n = len(target)\n",
    "    f1 = np.zeros(bootstrap_n)\n",
    "    for bs in np.arange(bootstrap_n):\n",
    "        i = np.random.choice(n, n, replace = True)\n",
    "        tp = pd.DataFrame({\"t\": target, \"p\": predict})\n",
    "        tp_bootstrap = tp.iloc[i, :]\n",
    "        f1[bs] = (f1_score(tp_bootstrap[\"t\"], tp_bootstrap[\"p\"], average='weighted')) #added , average='weighted'\n",
    "    res = [f1.mean(), np.quantile(f1,0.025), np.quantile(f1,0.975), f1.std()]\n",
    "    return res\n",
    "\n",
    "def precision_conf_int(target, predict, bootstrap_n = 500):\n",
    "    n = len(target)\n",
    "    p = np.zeros(bootstrap_n)\n",
    "    for bs in np.arange(bootstrap_n):\n",
    "        i = np.random.choice(n, n, replace = True)\n",
    "        tp = pd.DataFrame({\"t\": target, \"p\": predict})\n",
    "        tp_bootstrap = tp.iloc[i, :]\n",
    "        p[bs] = (precision_score(tp_bootstrap[\"t\"], tp_bootstrap[\"p\"], average='weighted')) #added , average='weighted'\n",
    "    res = [p.mean(), np.quantile(p,0.025), np.quantile(p,0.975), p.std()]\n",
    "    return res\n",
    "  \n",
    "def recall_conf_int(target, predict, bootstrap_n = 500):\n",
    "    n = len(target)\n",
    "    r = np.zeros(bootstrap_n)\n",
    "    for bs in np.arange(bootstrap_n):\n",
    "        i = np.random.choice(n, n, replace = True)\n",
    "        tp = pd.DataFrame({\"t\": target, \"p\": predict})\n",
    "        tp_bootstrap = tp.iloc[i, :]\n",
    "        r[bs] = (recall_score(tp_bootstrap[\"t\"], tp_bootstrap[\"p\"], average='weighted')) #added , average='weighted'\n",
    "    res = [r.mean(), np.quantile(r,0.025), np.quantile(r,0.975), r.std()]\n",
    "    return res\n",
    "\n",
    "def class_0_f1_conf_int(target, predict, bootstrap_n = 500):\n",
    "    n = len(target)\n",
    "    r = np.zeros(bootstrap_n)\n",
    "    for bs in np.arange(bootstrap_n):\n",
    "        i = np.random.choice(n, n, replace = True)\n",
    "        tp = pd.DataFrame({\"t\": target, \"p\": predict})\n",
    "        tp_bootstrap = tp.iloc[i, :]\n",
    "        r[bs] = (f1_score(tp_bootstrap[\"t\"], tp_bootstrap[\"p\"], pos_label=0, average='binary'))\n",
    "    res = [r.mean(), np.quantile(r,0.025), np.quantile(r,0.975), r.std()]\n",
    "    return res\n",
    "\n",
    "def class_1_f1_conf_int(target, predict, bootstrap_n = 500):\n",
    "    n = len(target)\n",
    "    r = np.zeros(bootstrap_n)\n",
    "    for bs in np.arange(bootstrap_n):\n",
    "        i = np.random.choice(n, n, replace = True)\n",
    "        tp = pd.DataFrame({\"t\": target, \"p\": predict})\n",
    "        tp_bootstrap = tp.iloc[i, :]\n",
    "        r[bs] = (f1_score(tp_bootstrap[\"t\"], tp_bootstrap[\"p\"], pos_label=1, average='binary'))\n",
    "    res = [r.mean(), np.quantile(r,0.025), np.quantile(r,0.975), r.std()]\n",
    "    return res\n",
    "\n",
    "def class_0_precision_conf_int(target, predict, bootstrap_n = 500):\n",
    "    n = len(target)\n",
    "    r = np.zeros(bootstrap_n)\n",
    "    for bs in np.arange(bootstrap_n):\n",
    "        i = np.random.choice(n, n, replace = True)\n",
    "        tp = pd.DataFrame({\"t\": target, \"p\": predict})\n",
    "        tp_bootstrap = tp.iloc[i, :]\n",
    "        r[bs] = (precision_score(tp_bootstrap[\"t\"], tp_bootstrap[\"p\"], pos_label=0, average='binary'))\n",
    "    res = [r.mean(), np.quantile(r,0.025), np.quantile(r,0.975), r.std()]\n",
    "    return res\n",
    "\n",
    "def class_1_precision_conf_int(target, predict, bootstrap_n = 500):\n",
    "    n = len(target)\n",
    "    r = np.zeros(bootstrap_n)\n",
    "    for bs in np.arange(bootstrap_n):\n",
    "        i = np.random.choice(n, n, replace = True)\n",
    "        tp = pd.DataFrame({\"t\": target, \"p\": predict})\n",
    "        tp_bootstrap = tp.iloc[i, :]\n",
    "        r[bs] = (precision_score(tp_bootstrap[\"t\"], tp_bootstrap[\"p\"], pos_label=1, average='binary'))\n",
    "    res = [r.mean(), np.quantile(r,0.025), np.quantile(r,0.975), r.std()]\n",
    "    return res\n",
    "\n",
    "def class_0_recall_conf_int(target, predict, bootstrap_n = 500):\n",
    "    n = len(target)\n",
    "    r = np.zeros(bootstrap_n)\n",
    "    for bs in np.arange(bootstrap_n):\n",
    "        i = np.random.choice(n, n, replace = True)\n",
    "        tp = pd.DataFrame({\"t\": target, \"p\": predict})\n",
    "        tp_bootstrap = tp.iloc[i, :]\n",
    "        r[bs] = (recall_score(tp_bootstrap[\"t\"], tp_bootstrap[\"p\"], pos_label=0, average='binary'))\n",
    "    res = [r.mean(), np.quantile(r,0.025), np.quantile(r,0.975), r.std()]\n",
    "    return res\n",
    "\n",
    "def class_1_recall_conf_int(target, predict, bootstrap_n = 500):\n",
    "    n = len(target)\n",
    "    r = np.zeros(bootstrap_n)\n",
    "    for bs in np.arange(bootstrap_n):\n",
    "        i = np.random.choice(n, n, replace = True)\n",
    "        tp = pd.DataFrame({\"t\": target, \"p\": predict})\n",
    "        tp_bootstrap = tp.iloc[i, :]\n",
    "        r[bs] = (recall_score(tp_bootstrap[\"t\"], tp_bootstrap[\"p\"], pos_label=1, average='binary'))\n",
    "    res = [r.mean(), np.quantile(r,0.025), np.quantile(r,0.975), r.std()]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 679
    },
    "id": "Mw0BPkukFbWA",
    "outputId": "0e3eaa34-6e5d-4f90-ef88-77d6197e32b2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fine tune Bert - update output_dir or say None\n",
    "bert_model = train_BERT(sentences=data_for_classifier['Text_snippet'].to_list(), labels=data_for_classifier['label'].to_list(), test_size=0.2, n_epochs=20, output_dir='./bert_base_v2_Jun26/')\n",
    "bert_model['stats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6bqCiEEYIIRd",
    "outputId": "77ffcbd3-30c0-40e7-facd-f8cbd5967f80",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run k-fold validation\n",
    "kf = BERT_KFOLD(sentences=data_for_classifier['Text_snippet'].to_list(), labels=data_for_classifier['label'].to_list(), n_splits=5, BERT_tokenizer=BERT_tokenizer, n_epochs=2,\n",
    "                random_state=666)\n",
    "print(kf['stats'], '\\n\\n', kf['stats_classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBLkcK-XIIIW"
   },
   "outputs": [],
   "source": [
    "kf['stats_classes'].to_csv('./bert_base_v2_Jun26/kf_stats_classes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ff9J5rjEIH_w"
   },
   "outputs": [],
   "source": [
    "kf['stats'].to_csv('./bert_base_v2_Jun26/kf_stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confidence intervals - this wont work here because the labels and pred variables are from the second bert method below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('./bert_base_v2_Jun26')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performance on test set\n",
    "\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(testing_data.shape[0]))\n",
    "\n",
    "# Create sentence and label lists\n",
    "sentences = testing_data.Text_snippet.values\n",
    "labels = testing_data.label.values\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 105,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 16  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions.\n",
    "      result = model(b_input_ids, \n",
    "                     token_type_ids=None, \n",
    "                     attention_mask=b_input_mask,\n",
    "                     return_dict=True)\n",
    "\n",
    "  logits = result.logits\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load the BERT model from a directory path\n",
    "model_path = \"./bert_base_v2_Jun26\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Move model to the desired device (e.g., GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Assuming you have your data in some format, load it here\n",
    "# Example:\n",
    "# test_data = load_test_data()\n",
    "\n",
    "# Assuming your data is in a DataFrame format with 'text' and 'label' columns\n",
    "# You can adjust this based on your actual data format\n",
    "# texts = test_data['text'].tolist()\n",
    "# labels = test_data['label'].tolist()\n",
    "texts = testing_data['Text_snippet'].tolist()\n",
    "labels = testing_data['label'].tolist()\n",
    "\n",
    "# Function to generate predictions using the BERT model\n",
    "def generate_predictions(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}  # Move inputs to the same device as the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "    predicted_labels = torch.argmax(probabilities, dim=1)\n",
    "    return predicted_labels.cpu().numpy()\n",
    "\n",
    "# Assuming you have loaded your test data and converted it into the required format\n",
    "# You can then generate predictions using the BERT model\n",
    "predicted_labels = generate_predictions(texts)\n",
    "\n",
    "# Assuming you have ground truth labels for your test data\n",
    "# You can then use the f1_conf_int function to calculate confidence intervals\n",
    "confidence_intervals = f1_conf_int(labels, predicted_labels)\n",
    "\n",
    "# Example usage:\n",
    "print(\"F1 score:\", confidence_intervals[0])\n",
    "print(\"95% CI lower bound:\", confidence_intervals[1])\n",
    "print(\"95% CI upper bound:\", confidence_intervals[2])\n",
    "print(\"Standard deviation:\", confidence_intervals[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the results across all batches. \n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_conf_int(labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1_conf_int(flat_true_labels, flat_predictions) #gives mean, lower ci, upper ci, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1_score(labels, predicted_labels,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "confidence_intervals = class_0_f1_conf_int(labels, predicted_labels)\n",
    "\n",
    "print(\"F1 score:\", confidence_intervals[0])\n",
    "print(\"95% CI lower bound:\", confidence_intervals[1])\n",
    "print(\"95% CI upper bound:\", confidence_intervals[2])\n",
    "print(\"Standard deviation:\", confidence_intervals[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "confidence_intervals = class_1_f1_conf_int(labels, predicted_labels)\n",
    "\n",
    "print(\"F1 score:\", confidence_intervals[0])\n",
    "print(\"95% CI lower bound:\", confidence_intervals[1])\n",
    "print(\"95% CI upper bound:\", confidence_intervals[2])\n",
    "print(\"Standard deviation:\", confidence_intervals[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision_confidence_intervals = precision_conf_int(labels, predicted_labels)\n",
    "\n",
    "print(\"Precision:\", precision_confidence_intervals[0])\n",
    "print(\"95% CI lower bound:\", precision_confidence_intervals[1])\n",
    "print(\"95% CI upper bound:\", precision_confidence_intervals[2])\n",
    "print(\"Standard deviation:\", precision_confidence_intervals[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision_confidence_intervals = class_0_precision_conf_int(labels, predicted_labels)\n",
    "\n",
    "print(\"Precision:\", precision_confidence_intervals[0])\n",
    "print(\"95% CI lower bound:\", precision_confidence_intervals[1])\n",
    "print(\"95% CI upper bound:\", precision_confidence_intervals[2])\n",
    "print(\"Standard deviation:\", precision_confidence_intervals[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision_confidence_intervals = class_1_precision_conf_int(labels, predicted_labels)\n",
    "\n",
    "print(\"Precision:\", precision_confidence_intervals[0])\n",
    "print(\"95% CI lower bound:\", precision_confidence_intervals[1])\n",
    "print(\"95% CI upper bound:\", precision_confidence_intervals[2])\n",
    "print(\"Standard deviation:\", precision_confidence_intervals[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recall_confidence_intervals = recall_conf_int(labels, predicted_labels)\n",
    "\n",
    "print(\"Recall:\", recall_confidence_intervals[0])\n",
    "print(\"95% CI lower bound:\", recall_confidence_intervals[1])\n",
    "print(\"95% CI upper bound:\", recall_confidence_intervals[2])\n",
    "print(\"Standard deviation:\", recall_confidence_intervals[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recall_confidence_intervals = class_0_recall_conf_int(labels, predicted_labels)\n",
    "\n",
    "print(\"Recall:\", recall_confidence_intervals[0])\n",
    "print(\"95% CI lower bound:\", recall_confidence_intervals[1])\n",
    "print(\"95% CI upper bound:\", recall_confidence_intervals[2])\n",
    "print(\"Standard deviation:\", recall_confidence_intervals[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recall_confidence_intervals = class_1_recall_conf_int(labels, predicted_labels)\n",
    "\n",
    "print(\"Recall:\", recall_confidence_intervals[0])\n",
    "print(\"95% CI lower bound:\", recall_confidence_intervals[1])\n",
    "print(\"95% CI upper bound:\", recall_confidence_intervals[2])\n",
    "print(\"Standard deviation:\", recall_confidence_intervals[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_report = classification_report(flat_true_labels, flat_predictions, output_dict=True) \n",
    "df_bert = pd.DataFrame(bert_report).transpose()\n",
    "df_bert.to_csv('./bert_base_Jun26/classification_report.csv', index= True)\n",
    "df_bert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_classifier.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_data.csv')\n",
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIObldsWEcg_"
   },
   "outputs": [],
   "source": [
    "# RUN PRETRAINED BERT AND ANALYSE ERRORS \n",
    "BERT_tokenizer = 'bert-base-uncased' #or can be 'bert-base-uncased'\n",
    "# model_path = data_path+'bert_models/'+dom+'_biobert'\n",
    "model_path = './bert_base_v2_Jun26' #update accordingly\n",
    "#text_col = 'clean_text'\n",
    "text_col = 'Text_snippet'\n",
    "\n",
    "#df = pd.read_excel(data_path+'/cognition_adjudicated_clean.xlsx')\n",
    "df = pd.read_csv('test_data.csv', usecols=['brcid', 'CN_Doc_ID', 'Text_snippet','label' ])\n",
    "                 #columns=['Brcid', 'CN_Doc_ID','doc_date', 'doc_type', 'kw', 'domain', 'start_kw', 'clean_text'])\n",
    "# df = df.loc[df.domain == dom].reset_index()\n",
    "df[text_col] = df[text_col].apply(lambda x: str(x).strip().lower())\n",
    "\n",
    "res= load_and_run_BERT(sentences=df[text_col]\n",
    "                       , trained_bert_model=model_path\n",
    "                       , BERT_tokenizer=BERT_tokenizer)\n",
    "\n",
    "\n",
    "res.to_csv('./bert_base_v2_Jun26_testdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpt=0\n",
    "BERT_tokenizer = 'bert-base-uncased' #'bert-base-uncased'\n",
    "#BERT_tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased') #'bert_base_uncased' #this seems to give a url error? and link that asks to sign in to huggingface\n",
    "#model_path = data_path+'/bert_models/cognition_all_biobert_rel10'\n",
    "model_path = './bert_base_v2_Jun26' #'./bert_base_model_nov14_second_script_notoverfit'\n",
    "#names=['BrcId', 'CN_Doc_ID','doc_date', 'doc_type', 'kw', 'domain', 'start_kw', 'clean_text']#, 'age', 'ethnicitycleaned', 'Gender_ID']\n",
    "names=['brcid', 'CN_Doc_ID', 'Text_snippet','label']\n",
    "for chunk in pd.read_csv('./test_data.csv'\n",
    "                         , chunksize=10000): #, names=names):\n",
    "    cpt+=1\n",
    "    print('chunk nb',cpt)\n",
    "    res= load_and_run_BERT(sentences=chunk['Text_snippet'] #['clean_text']\n",
    "                   , trained_bert_model=model_path\n",
    "                   , BERT_tokenizer=BERT_tokenizer)\n",
    "\n",
    "    result = pd.concat([res.reset_index()[['preds','probs']]\n",
    "                        , chunk.reset_index()[['brcid', 'CN_Doc_ID', 'Text_snippet', 'label']]] #'BrcId', 'CN_Doc_ID','doc_date', 'doc_type', 'kw', 'domain', 'start_kw', 'clean_text']]]\n",
    "                       , axis=1)\n",
    "    print(result.head(1))\n",
    "    header_bool=1 if cpt==1 else False\n",
    "    result[['preds','probs']+names].to_csv('./test_data_bert_base_v2_Jun26.csv', mode='a', header=header_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test_with_clean_text.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "samplesize_build_classifiers_incl_BERT.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06b221ddb5fd49249040f1e55b74d228": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "093b07e2911746a7b4a5760bbf983003": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_768827fd1bec42f599c9a127e622b016",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_57c72f3ef0e24c45853fbe5ac944fac2",
      "value": 28
     }
    },
    "0c8cc7297c2240a2b882436d9d20b9f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0cd2c99160f8438480f4b0a65afea8ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f237212ac864b3294f5cb3fddeb9fb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1670eef7f5264c8193441ad4d00e77f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "18032b86ea1f465385792529ba1578af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32903a36fc1a450fbc409ac0b6b5f3b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32cd4c65e8b046b1bf664ca0dc983136": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "407c40cd8c284d10b5b3b5699888b611": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ddd73e659c134b07aca49a04a3973f52",
       "IPY_MODEL_425ea3669a7446a8ad5f76fd0beca284",
       "IPY_MODEL_c6415b02b9254825b9e924fed751774a"
      ],
      "layout": "IPY_MODEL_abaed86415d348da9bc38c0b10f2ed49"
     }
    },
    "425ea3669a7446a8ad5f76fd0beca284": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_db3dd9d756ab4e419557a1a5fedd3de5",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_06b221ddb5fd49249040f1e55b74d228",
      "value": 231508
     }
    },
    "438a048d82a74b84acd8b27a2f9073cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4403a81740bd47bebc8a3c330090a297": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "492ae1237b724366aadfaf083c132784": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5abe141de9944255ba3b000546152c2d",
       "IPY_MODEL_65bb3aa2c4da4f05b67334880a8c82ac",
       "IPY_MODEL_fa4547987b4649dfb0654d938672e9d3"
      ],
      "layout": "IPY_MODEL_f34e3b80a1fb42138af5bbdd1d437272"
     }
    },
    "4bcece6c94f54573bb2c0a925fd3cddc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8595bbbf16c94f37a297e35cac21264b",
       "IPY_MODEL_c9004b62a6644142ad7c398be1cdee9b",
       "IPY_MODEL_cf5382d3c2ed4feabb63e7c9a33d09b9"
      ],
      "layout": "IPY_MODEL_32cd4c65e8b046b1bf664ca0dc983136"
     }
    },
    "5670d4b4e8894ff38b338518c37e3ec1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "57c72f3ef0e24c45853fbe5ac944fac2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "59f9889cf43a43e19a67067495887400": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bd3eda2c1abb4f49b454a6dbb6af93ec",
       "IPY_MODEL_093b07e2911746a7b4a5760bbf983003",
       "IPY_MODEL_f951c4a9618548cbb57f388966ebf034"
      ],
      "layout": "IPY_MODEL_8a9ab6e29aa44ed0a1666b54ab8f5c09"
     }
    },
    "5abe141de9944255ba3b000546152c2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d6c6377d50d646b1b489a2074a39183c",
      "placeholder": "",
      "style": "IPY_MODEL_6c13f58d2f0a47ca9d23383c45d0fc54",
      "value": "Downloading: 100%"
     }
    },
    "60a21d70be2c4fa9abe8b376cfb5f41f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "65bb3aa2c4da4f05b67334880a8c82ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b48d90386454488f98c7d733654fca2e",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7feb965a03d54caab36b47dc80192d1b",
      "value": 466062
     }
    },
    "6776154723f54c9d92f3ade98426607c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "67f2428d5ace47fbbdb6b88b1b87ef22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6b4816403a39414588ffe530732862c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6c13f58d2f0a47ca9d23383c45d0fc54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6d923a2acb064161850fdb740e8cfaa7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "768827fd1bec42f599c9a127e622b016": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7999f514a67b4b8a91c9e7dfb1c5db9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7feb965a03d54caab36b47dc80192d1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8595bbbf16c94f37a297e35cac21264b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32903a36fc1a450fbc409ac0b6b5f3b1",
      "placeholder": "",
      "style": "IPY_MODEL_0f237212ac864b3294f5cb3fddeb9fb7",
      "value": "Downloading: 100%"
     }
    },
    "85ee92792a9247ba813ff6d3e35e50f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "86fe03383ae347708284fbd4d4ad4659": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_438a048d82a74b84acd8b27a2f9073cd",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7999f514a67b4b8a91c9e7dfb1c5db9a",
      "value": 570
     }
    },
    "874c1416b7034814929f54fa099e0bc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8a9ab6e29aa44ed0a1666b54ab8f5c09": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "987c7ebb9b1a45149ddc9e050d965e88": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abaed86415d348da9bc38c0b10f2ed49": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad5b6b6f43864a219bb5b573375e4459": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d53b0be5499a4560ad318ddefdfb94ec",
       "IPY_MODEL_86fe03383ae347708284fbd4d4ad4659",
       "IPY_MODEL_d8993681a7274d7699ad112cf8a5142f"
      ],
      "layout": "IPY_MODEL_d70f2b0dd02d444f8b883896e1351b8f"
     }
    },
    "b0c1aea6a8934007a582a01dcef83cd4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b48d90386454488f98c7d733654fca2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bce10f3bb5584faab6ab8d1d33ccccd1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd3eda2c1abb4f49b454a6dbb6af93ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc4efddb180d4f17945f7ee700d10044",
      "placeholder": "",
      "style": "IPY_MODEL_85ee92792a9247ba813ff6d3e35e50f3",
      "value": "Downloading: 100%"
     }
    },
    "c6415b02b9254825b9e924fed751774a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b0c1aea6a8934007a582a01dcef83cd4",
      "placeholder": "",
      "style": "IPY_MODEL_0c8cc7297c2240a2b882436d9d20b9f9",
      "value": " 232k/232k [00:00&lt;00:00, 854kB/s]"
     }
    },
    "c9004b62a6644142ad7c398be1cdee9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1670eef7f5264c8193441ad4d00e77f1",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6b4816403a39414588ffe530732862c9",
      "value": 440473133
     }
    },
    "cf5382d3c2ed4feabb63e7c9a33d09b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0cd2c99160f8438480f4b0a65afea8ba",
      "placeholder": "",
      "style": "IPY_MODEL_d51957f0c02f457c9c80c3e112bf58c2",
      "value": " 440M/440M [00:07&lt;00:00, 58.1MB/s]"
     }
    },
    "d51957f0c02f457c9c80c3e112bf58c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d53b0be5499a4560ad318ddefdfb94ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4403a81740bd47bebc8a3c330090a297",
      "placeholder": "",
      "style": "IPY_MODEL_5670d4b4e8894ff38b338518c37e3ec1",
      "value": "Downloading: 100%"
     }
    },
    "d6c6377d50d646b1b489a2074a39183c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d70f2b0dd02d444f8b883896e1351b8f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8993681a7274d7699ad112cf8a5142f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_18032b86ea1f465385792529ba1578af",
      "placeholder": "",
      "style": "IPY_MODEL_60a21d70be2c4fa9abe8b376cfb5f41f",
      "value": " 570/570 [00:00&lt;00:00, 5.16kB/s]"
     }
    },
    "db3dd9d756ab4e419557a1a5fedd3de5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc4efddb180d4f17945f7ee700d10044": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ddd73e659c134b07aca49a04a3973f52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_987c7ebb9b1a45149ddc9e050d965e88",
      "placeholder": "",
      "style": "IPY_MODEL_67f2428d5ace47fbbdb6b88b1b87ef22",
      "value": "Downloading: 100%"
     }
    },
    "f34e3b80a1fb42138af5bbdd1d437272": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f951c4a9618548cbb57f388966ebf034": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bce10f3bb5584faab6ab8d1d33ccccd1",
      "placeholder": "",
      "style": "IPY_MODEL_874c1416b7034814929f54fa099e0bc8",
      "value": " 28.0/28.0 [00:00&lt;00:00, 926B/s]"
     }
    },
    "fa4547987b4649dfb0654d938672e9d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d923a2acb064161850fdb740e8cfaa7",
      "placeholder": "",
      "style": "IPY_MODEL_6776154723f54c9d92f3ade98426607c",
      "value": " 466k/466k [00:00&lt;00:00, 888kB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
